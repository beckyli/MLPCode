{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n",
      "['disk', 'format', 'hard', 'how', 'my', 'problem', 'to']\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['02.txt', '05.txt', '01.txt', '04.txt', '03.txt']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "### First Similarity Measurement \n",
    "#################################\n",
    "\n",
    "#################################\n",
    "### ??? Normailized and raw ??? \n",
    "#################################\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1,v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "# Normalizing word count vectors\n",
    "# calculate the vector distance not on the raw vectors but on the normalized instead\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "# When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "# 只考虑词汇在文本中出现的频率\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problem \"]\n",
    "# fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。\n",
    "X = vectorizer.fit_transform(content)\n",
    "print(X)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "# transpose 数组转置\n",
    "print(X.toarray().transpose())\n",
    "\n",
    "os.listdir('./data/toy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='red'>fit and transform</font> \n",
    "<ol>\n",
    "  <li>fit: 求得训练集X的均值，方差，最大值，最小值,这些训练集X固有的属性</li>\n",
    "  <li>transform: 在fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）</li>\n",
    "  <li>fit_transform: fit_transform是fit和transform的组合，既包括了训练又包含了转换</li>\n",
    "  <li>This is question two.</li>\n",
    "</ol>\n",
    "\n",
    "## <font color='red' CountVectorizer>CoundVectorizer </font>\n",
    "CountVectorizer举例，sklearn的CountVectorizer库是根据输入数据获取词频矩阵（稀疏矩阵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imaging databases provide storage capabilities.', 'Imaging databases store data. Imaging databases store data. Imaging databases store data.', 'This is a toy post about machine learning. Actually, it contains not much interesting stuff.', 'Imaging databases store data.', 'Most imaging databases save images permanently.\\n']\n",
      "#samples: 5, #features: 17\n",
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'save', 'storag', 'store', 'stuff', 'toy']\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "  (0, 4)\t0.7071067811865475\n",
      "  (0, 5)\t0.7071067811865475\n",
      "[[0.         0.         0.         0.         0.70710678 0.70710678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# read files\n",
    "DIR = \"./data/toy/\"\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
    "print(posts)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features))\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)\n",
    "print(new_post_vec.toarray())\n",
    "new_post_vec_normalized = new_post_vec/sp.linalg.norm(new_post_vec.toarray())\n",
    "print(new_post_vec_normalized)\n",
    "print(new_post_vec_normalized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Numpy linalg 库</font>\n",
    "向量范数、矩阵范数、行列式、矩阵逆、幂\n",
    "numpy.linalg模块包含线性代数的函数。使用这个模块，我们可以计算逆矩阵、求特征值、解线性方程组以及求解行列式等。\n",
    "## <font color='red'>sys.maxsize:</font>\n",
    "## <font color='red'>() and []</font>\n",
    "## <font color='red'>* and **: 打包和解包</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=0.86:Imaging databases provide storage capabilities.\n",
      "=== Post 1 with dist=0.77:Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 2 with dist=1.41:This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 3 with dist=0.77:Imaging databases store data.\n",
      "=== Post 4 with dist=0.63:Most imaging databases save images permanently.\n",
      "\n",
      "Best post is 4 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "#for i, post in enumerate(num_samples):\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "    print(\"=== Post %i with dist=%.2f:%s\"%(i,d,post))\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove less import words --> \"test processing\"\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")\n",
    "s.stem(\"imagine\")\n",
    "s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the vectorizer with NLTK's stemmer\n",
    "# super 用于调用父类(超类)的一个方法\n",
    "# Python使用lambda关键字创造匿名函数  \n",
    "\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
